<!doctype html><html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Face identification with Azure Cognitive Services Face API"><meta property="og:description" content="Recently I&rsquo;ve been playing with a lot AI and seeing how it can be used in mobile apps to enhance the experience offered to the user. Currently I am playing with facial recognition using the Azure Cognitive Services Face API. This is a stupidly powerful API that can do a LOT of different things:
Detect faces in images Analyze those faces to detect characteristics such as hair color, gender, age Detect the different points on a face, such as the pupils or lips Compare two faces and verify if they are from the same person Group faces by similar facial characteristics Identify faces from a repository of up to a million images This is a very powerful set of APIs with a large number of different use cases."><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/face-identification-with-azure-faceapi/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/face-identification-with-azure-faceapi/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2018-03-02T20:27:18+00:00"><meta property="article:modified_time" content="2018-03-02T20:27:18+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/face-identification-with-azure-faceapi/banner.png"><meta name=twitter:title content="Face identification with Azure Cognitive Services Face API"><meta name=twitter:description content="Recently I&rsquo;ve been playing with a lot AI and seeing how it can be used in mobile apps to enhance the experience offered to the user. Currently I am playing with facial recognition using the Azure Cognitive Services Face API. This is a stupidly powerful API that can do a LOT of different things:
Detect faces in images Analyze those faces to detect characteristics such as hair color, gender, age Detect the different points on a face, such as the pupils or lips Compare two faces and verify if they are from the same person Group faces by similar facial characteristics Identify faces from a repository of up to a million images This is a very powerful set of APIs with a large number of different use cases."><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><link rel=alternate type=application/rss+xml href=https://jimbobbennett.dev/index.xml title=JimBobBennett><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><script data-goatcounter=https://jimbobbennett.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Face identification with Azure Cognitive Services Face API | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/podcasts title=Podcasts>Podcasts</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Face identification with Azure Cognitive Services Face API</h1><div class="text-left content"><a href=https://linkedin.com/in/jimbobbennett>Jim Bennett
</a><small>|</small>
Mar 2, 2018</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>Recently I&rsquo;ve been playing with a lot AI and seeing how it can be used in mobile apps to enhance the experience offered to the user. Currently I am playing with facial recognition using the <a href=https://aka.ms/Vd514y>Azure Cognitive Services Face API</a>. This is a stupidly powerful API that can do a LOT of different things:</p><ul><li>Detect faces in images</li><li>Analyze those faces to detect characteristics such as hair color, gender, age</li><li>Detect the different points on a face, such as the pupils or lips</li><li>Compare two faces and verify if they are from the same person</li><li>Group faces by similar facial characteristics</li><li>Identify faces from a repository of up to a million images</li></ul><p>This is a very powerful set of APIs with a large number of different use cases. For example if you were building a social network you could use the facial identification to automatically tag peoples friends in images. If you were building a ride share system you could use facial verification to ensure the driver is who you expect it to be. For now I&rsquo;m going to focus on one particular example - identifying faces from a photo in a mobile app.</p><h2 id=face-finder>Face Finder</h2><p>I&rsquo;ve built a sample mobile app to show off the facial recognition tools in this API, and you can grab the code from <a href=https://github.com/jimbobbennett/FaceFinder>my GitHub repo</a>. This app takes a photo, then finds all the faces in that photo, giving a breakdown of the details of each face.</p><div class=image-div style=max-width:300px><p><img src=FaceAPI.gif alt="Animated GIF of the Face Finder app in action"></p></div><p>In the rest of this post I&rsquo;ll go through you can get signed up for FaceAPI, and how the app works.</p><h4 id=getting-started-with-faceapi>Getting started with FaceAPI</h4><p>From the <a href="https://portal.azure.com/?WT.mc_id=azureportal-blog-jabenn">Azure Portal</a>, select <strong>Create a resource</strong>, search for <em>&ldquo;Face&rdquo;</em>, and select <strong>Face</strong> from the <strong>AI + Machine Learning</strong> category. The click <strong>Create</strong>.</p><div class=image-div style=max-width:480px><p><img src=2018-11-05_11-20-49.png alt="Selecting the Face API"></p></div><p>Enter a name for this resource, select your subscription and the location nearest to you. For the pricing tier, there is a free tier called <strong>F0</strong> that gives you 30,000 calls per month at a rate of no more than 20 per minute, and you can have one face resource per subscription with this tier. After this there is a paid tier limited to 10 calls per second and you pay per 1,000 calls - at the time of writing this is US$0.25 per 1,000 calls.</p><div class=image-div style=max-width:300px><p><img src=2018-11-05_11-24-47.png alt="The pricing matrix for Face calls"></p></div><p>Choose a resource group, or create a new one and click <strong>Create</strong>. Once it has been created, head to it and grab an API key from <em>Resource Management->Keys</em>, and the endpoint from the <em>Overview</em> blade.</p><h4 id=building-and-running-the-app>Building and running the app</h4><p>The Face Finder app is pretty complete, all you need to do is update the <code>ApiKeys.cs</code> file with your API key and endpoint. For the <code>FaceApiKey</code>, copy yours and paste it in. For the <code>FaceApiEndpoint</code>, paste the value for the endpoint, removing everything past <code>microsoft.com</code>. For example, for me the endpoint shown in the Azure portal is <em><a href=https://westeurope.api.cognitive.microsoft.com/face/v1.0>https://westeurope.api.cognitive.microsoft.com/face/v1.0</a></em>, so I would set the endpoint to <code>https://westeurope.api.cognitive.microsoft.com</code>.</p><blockquote><p>If you get a <em>Not Found</em> exception, then check your endpoint - this exception is thrown if you don&rsquo;t remove everything past <code>microsoft.com</code>.</p></blockquote><p>Once you have done this, build and run the app. When it loads, tap the <strong>Take photo</strong> button and take a picture of one or more faces. The app will then show a list of all the faces detected, describing them using the detected age and gender. Tap on a face in the list to see more details, including if that person is smiling, if they are wearing glasses, what hair, facial hair and makeup they have, and their emotion.</p><h4 id=so-how-does-it-work>So how does it work</h4><p>This app is a simple Xamarin.Forms app, with three pages and some view models. The first page, <code>FaceFinderPage.xaml</code> has a button you tap to take a photo, wired up to a command on the <code>FaceFinderViewModel</code>. This uses the <a href=https://www.nuget.org/packages/Xam.Plugin.Media/>Xam.Plugin.Media</a> plugin from <a href=https://twitter.com/JamesMontemagno>James Montemagno</a> to launch the camera and take a picture. This picture is then run through the face API.</p><p>The face API is accessed via an SDK from a NuGet package. Currently there are a load of NuGet packages from Microsoft with names containing <strong>ProjectOxford</strong> - the code name for the various vision cognitive services. These are being replaced with new packages that are called <strong>Microsoft.Azure.CognitiveServices.*</strong>, and these packages are currently in pre-release. For the face API, I&rsquo;m using the <strong>Microsoft.Azure.CognitiveServices.Vision.Face</strong> package, currently available as a pre-release package.</p><div class=image-div style=max-width:600px><p><img src=2018-11-05_11-34-13.png alt="Adding the Vision.Face package"></p></div><p>The important class to note here is <code>FaceClient</code>, this wraps a connection to the Azure Face Api and is configured using your API key and endpoint.</p><p><strong>Initializing the FaceClient</strong></p><p>Before you can use the Face API you have to configure it to use one of your keys and the appropriate endpoint. When constructing an instance of <code>FaceClient</code> you need to pass it credentials, in the form of an instance of <code>ApiKeyServiceClientCredentials</code> which takes one of the API keys assigned to your account as a string in a constructor argument:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> creds = <span style=color:#66d9ef>new</span> ApiKeyServiceClientCredentials(<span style=color:#e6db74>&#34;&lt;your api key&gt;&#34;</span>);
</span></span></code></pre></div><p>You can then pass this to the constructor of the <code>FaceClient</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> faceClient = <span style=color:#66d9ef>new</span> FaceClient(creds);
</span></span></code></pre></div><p>Finally you set the appropriate endpoint to match the endpoint shown with your keys:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span>faceClient.Endpoint = <span style=color:#e6db74>&#34;https://westeurope.api.cognitive.microsoft.com&#34;</span>;
</span></span></code></pre></div><p><strong>Detecting faces</strong></p><p>Once you have your instance of the <code>FaceClient</code> you can then use that to detect faces using the <code>Face</code> property to access all the different face operations the API supports. The method I&rsquo;m interested in is the <code>DetectWithStreamAsync</code> method. This takes a stream containing the image, and sends it up to Azure to detect faces. The <code>Async</code> suffix is because it is an <code>async</code> method that you can await (not sure why they&rsquo;ve added this suffix - they don&rsquo;t have non-async versions on the API).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> faces = faceClient.Face.DetectInStreamAsync(imageStream);
</span></span></code></pre></div><p>The <code>imageStream</code> comes from the media plugin. When you use this plugin to take a photo it returns a <code>MediaFile</code> which has a method to get the image as a stream that can be passed to the detect call. This method has some other parameters on it which we&rsquo;ll look at later.</p><p>The results of this call is a list of detected faces - literally a <code>List&lt;DetectedFace></code>, with one entry per face that was detected in the image. Each <code>DetectedFace</code> in the list contains a set of properties about that face, including coordinates of a rectangle that shows where the face is in the image. The picture below shows a face with this rectangle drawn on top.</p><div class=image-div style=max-width:300px><p><img src=2018-03-02_17-08-06.png alt="A picture of the author with a bounding box showing the face rectangle"></p></div><p><strong>Detecting face attributes</strong></p><p>So far, so good - we can find where a face is. Now what about more details about the face? This is where the extra parameters on the <code>DetectWithStreamAsync</code> method come in.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span>DetectWithStreamAsync(Stream image, 
</span></span><span style=display:flex><span>                      <span style=color:#66d9ef>bool?</span> returnFaceId = <span style=color:#66d9ef>true</span>, 
</span></span><span style=display:flex><span>                      <span style=color:#66d9ef>bool?</span> returnFaceLandmarks = <span style=color:#66d9ef>false</span>, 
</span></span><span style=display:flex><span>                      IList&lt;FaceAttributeType&gt; returnFaceAttributes = <span style=color:#66d9ef>null</span>, 
</span></span><span style=display:flex><span>                      CancellationToken cancellationToken = <span style=color:#66d9ef>default</span>(CancellationToken));
</span></span></code></pre></div><p>So what do these parameters do:</p><ul><li><code>returnFaceId</code> - set this to <code>true</code> (the default) to return an Id for the face. This Id can then be used for face searching operations - outside the scope of this post!</li><li><code>returnFaceLandmarks</code> - set this to <code>true</code> (default is <code>false</code>) to return the coordinates of the facial landmarks, for example the positions of the pupils, nose, lips etc.</li><li><code>returnFaceAttributes</code> - this is a list of the different face attributes you want returned. There are a lot of these! In the Face Finder app I get them all, and they are:<ul><li><code>Age</code> - a guess at the age of the face. Seeing as it predicted me at 9 years older than I am in the image above it&rsquo;s either buggy, or (more likely) I need more sleep and to look after myself!</li><li><code>Gender</code> - a guess at the presented gender. Just limited to male or female.</li><li><code>HeadPose</code> - what position the head is in, pitch, roll and yaw.</li><li><code>Smile</code> - the percentage certainty that the face is smiling.</li><li><code>FacialHair</code> - the percentage certainty that the face has a beard, mustache or sideburns.</li><li><code>Glasses</code> - the type of glasses (if any) the person is wearing, so normal glasses, sun glasses etc.</li><li><code>Emotion</code> - the percentages that the face is displaying a set of emotions (e.g. anger, happiness, surprise)</li><li><code>Hair</code> - the percentages certainty that the face has different colored hair. If no hair is detected then this list is empty, otherwise it covers all natural hair colors and &lsquo;other&rsquo;. This also specifies if the face is bald or if the hair is invisible (such as under a hat or scarf).</li><li><code>Makeup</code> - The percentage certainty that the face has eye or lip makeup on.</li><li><code>Occlusion</code> - how much of the face is occluded (such as by a mask, bandana, hair etc.)</li><li><code>Accessories</code> - any accessories on the face, such as glasses or a hat</li><li><code>Blur</code> - how blurry the face is.</li><li><code>Exposure</code> - how well exposed the picture is.</li><li><code>Noise</code> - how much noise there is in the image.</li></ul></li></ul><p>These landmarks and attributes come back on the <code>DetectedFace</code> instances, and are present for all faces in the image.</p><h4 id=what-sort-of-apps-can-i-build-with-this>What sort of apps can I build with this?</h4><p>This API is great and can provide a LOT of power, but are they just for mucking around, or can we build real-world apps with them? Well a few examples I can think of without trying too hard are:</p><ul><li>Passport photo app. Passports have strict requirements about photos, so you could use this API to ensure the person was looking at the camera (<code>HeadPose</code>), no glasses (<code>Glasses</code>) or make-up (<code>Makeup</code>), and a neutral expression (<code>Emotion</code>). The photo needs to be of good quality (<code>Exposure</code>, <code>Blur</code>, <code>Noise</code>).</li><li>Facial blurring. You could detect the face rectangle and blur out faces automatically, for example in apps that make photos public. You could even do it by age to only blur out children if needed.</li><li>Adding decoration to faces. Using the landmarks you could add fake glasses, hats, dog noses or other things to a face to make a comedy picture. I&rsquo;ve even seen an example replacing faces with emojis that match the emotion being shown.</li><li>Auto-picture selection. You could take a selection of pictures and have it choose the best one based on quality (<code>Exposure</code>, <code>Blur</code>, <code>Noise</code>) and if the people in it are smiling (<code>Emotion</code>).</li></ul><h4 id=where-can-i-learn-more>Where can I learn more?</h4><p>We&rsquo;ve got loads of great content on-line showing all the cool things you ca do with this API, from all different languages. Check them out:</p><ul><li>Read the offical docs at <a href=https://aka.ms/I5y8t7>docs.microsoft.com</a></li><li>Work through some great tutorials from the <a href=https://aka.ms/Hz3eo7>Microsoft AI School</a></li></ul></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents><ul><li><a href=#face-finder>Face Finder</a><ul><li></li></ul></li></ul></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin target=_blank>xamarin</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ai target=_blank>AI</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/azure target=_blank>azure</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin.forms target=_blank>xamarin.forms</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/nuget target=_blank>nuget</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/technology target=_blank>technology</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/api target=_blank>api</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/faceapi target=_blank>faceapi</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/face-identification-with-azure-faceapi/",this.page.identifier="9140df3e40a8f5f1c077f40cded76c1a"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2024 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script><script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>