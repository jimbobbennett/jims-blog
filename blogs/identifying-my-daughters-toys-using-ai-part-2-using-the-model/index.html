<!doctype html><html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Identifying my daughters toys using AI - Part 2, using the model"><meta property="og:description" content="In the first part of this post I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
Although this works, it&rsquo;s a long winded way of using the model. Ideally I&rsquo;d want it in a mobile app (after all, I am a bit of a Xamarin fan) so in this post we&rsquo;ll look at calling the classifier from a Xamarin app."><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2018-01-02T17:43:37+00:00"><meta property="article:modified_time" content="2018-01-02T17:43:37+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/banner.png"><meta name=twitter:title content="Identifying my daughters toys using AI - Part 2, using the model"><meta name=twitter:description content="In the first part of this post I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
Although this works, it&rsquo;s a long winded way of using the model. Ideally I&rsquo;d want it in a mobile app (after all, I am a bit of a Xamarin fan) so in this post we&rsquo;ll look at calling the classifier from a Xamarin app."><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><link rel=alternate type=application/rss+xml href=https://jimbobbennett.dev/index.xml title=JimBobBennett><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><script data-goatcounter=https://jimbobbennett.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Identifying my daughters toys using AI - Part 2, using the model | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Identifying my daughters toys using AI - Part 2, using the model</h1><div class="text-left content"><a href=https://linkedin.com/in/jimbobbennett>Jim Bennett
</a><small>|</small>
Jan 2, 2018</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai/>first part of this post</a> I used the <a href="http://customvision.ai/?wt.mc_id=toyidentifier-blog-jabenn">Azure Custom Vision service</a> to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.</p><p>Although this works, it&rsquo;s a long winded way of using the model. Ideally I&rsquo;d want it in a mobile app (after all, I am a bit of a Xamarin fan) so in this post we&rsquo;ll look at calling the classifier from a Xamarin app.</p><p>The first thing to do is to create a new blank Xamarin Forms app, I&rsquo;m going to call mine <strong>Toy Identifier</strong>. This app will be pretty simple to start with - one screen with a big button to launch the camera, then after a photo is take I&rsquo;ll use text to speech to say the name of the toy.</p><div class=image-div style=max-width:800px><p><img src=2017-12-27_16-39-46.png alt="The main screen launches the camera to identify the toy"></p></div><p>Like a lot of Xamarin apps, the bulk of this will just be some of James Montemagno&rsquo;s Plugins joined together, so add the following packages to all the projects (core, iOS and Android):</p><ul><li><code>Xam.Plugin.Media</code></li><li><code>Xam.Plugins.TextToSpeech</code></li></ul><p>The Media plugin has some permissions you need to set, so follow the instructions at <a href=https://github.com/jamesmontemagno/MediaPlugin>https://github.com/jamesmontemagno/MediaPlugin</a> to configure this.</p><p>We&rsquo;ll also need the Custom Vision client side SDKs, so add:</p><ul><li><code>Microsoft.Cognitive.CustomVision.Prediction</code></li><li><code>Microsoft.Rest.ClientRuntime</code></li></ul><p>as well to all your projects.</p><p>You can find all the source code for this app on GitHub at <a href=https://github.com/jimbobbennett/ToyIdentifier>https://github.com/jimbobbennett/ToyIdentifier</a>, so grab the code now.</p><p>The UI is simple - a button and a label. Tap the button and the camera appears, take a photo and the label shows the name of the toy. I am using MVVM, but not bothering with a framework at this point as the app is too simple, so I&rsquo;ve got a view model that is created inside my XAML which has all the fun logic.</p><p>If you open <code>ToyIdentifierViewModel.cs</code> you&rsquo;ll see how simple it is to take photos and upload them to the custom vision service. Taking a photo is easy thanks to James Montemagno&rsquo;s Media plugin:</p><pre tabindex=0><code>var options = new StoreCameraMediaOptions { PhotoSize = PhotoSize.Medium };
var file = await CrossMedia.Current.TakePhotoAsync(options);
</code></pre><p>I&rsquo;m using a medium sized photo - this returns an image which is 50% of the size of the one that comes out of the camera. This is fine as we don&rsquo;t need a high res image for recognition, and the serviced apartment I&rsquo;m staying in at the moment has crappy internet so the smaller the better.</p><p>The <code>TakePhotoAsync</code> call returns a <code>MediaFile</code> object, with a method on it to return the file as a stream, and we can pass this stream to a call to the custom vision SDK.</p><p>To use the custom vision SDK we need to start by creating a <code>PredictionEndpoint</code>. Uploading an image to get the percentage chance of it matching the tags is called prediction.</p><pre tabindex=0><code>private PredictionEndpoint _endpoint = new PredictionEndpoint { ApiKey = ApiKeys.PredictionKey };
</code></pre><p>The <code>PredictionKey</code> I&rsquo;m passing in comes from a constants file - you&rsquo;ll need to update this file with your own API key. You can find this by going to the settings on your project in the custom vision portal and getting the value of the <strong>Prediction Key</strong>. There is also a <code>ProjectId</code> in that same file you need to set, again this comes from the settings page.</p><p>Using the prediction endpoint is easy - there is a <code>PredictImage</code> method on this class, and this takes the project id as a Guid, and a stream containing the image to use for the prediction.</p><pre tabindex=0><code>_endpoint.PredictImage(ApiKeys.ProjectId, stream)
</code></pre><p>This call uploads the image to the custom vision service and returns a model containing information about the custom vision project, and a list of predictions - essentially a list of all the tags the a numerical probability that the image matches the tag in the range of 0 to 1, the higher the number, the more likely the match.</p><p>Once we have these predictions we can sort them by probability then get the highest. We then need to add a threshold - after all if we match all tags with a probability &lt; 0.01 then it&rsquo;s unlikely the image is one of our tags. In my code I&rsquo;ve got a threshold of 0.5 to start with, but I might increase this after playing some more.</p><pre tabindex=0><code>_endpoint.PredictImage(ApiKeys.ProjectId, stream)
         .Predictions
         .OrderByDescending(p =&gt; p.Probability)
         .FirstOrDefault(p =&gt; p.Probability &gt; 0.5);
</code></pre><p>Once I have the best match I update the label to say hello to the particular toy, then use the text to speech plugin from James Montemagno to actually say hello to the toy. If there are no matches the label is changed to say &ldquo;I don&rsquo;t know who that is&rdquo;.</p><p>If you want to see more on the SDK, it is on GitHub at <a href=https://github.com/Microsoft/Cognitive-CustomVision-Windows>https://github.com/Microsoft/Cognitive-CustomVision-Windows</a>. At the time of writing the code in there is a bit out of date and doesn&rsquo;t match the NuGet package, so check out <a href=https://github.com/Microsoft/Cognitive-CustomVision-Windows/pull/11>this pull request</a> to find the latest code.</p><p>Check out this short demo:</p><div class=image-div style=max-width:300px><p><img src=Porg.gif alt="Demo of identifying a Porg"></p></div><p>Want to read more on how to get started with the Custom Vision APIs? Check out the docs <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home?wt.mc_id=toyidentifier-blog-jabenn">here</a>.</p><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/>next post</a> we&rsquo;ll look at exporting these models to run on an iOS device.</p></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/azure target=_blank>azure</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/technology target=_blank>technology</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin target=_blank>xamarin</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin.forms target=_blank>xamarin.forms</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/porg target=_blank>Porg</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/custom-vision target=_blank>custom vision</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/",this.page.identifier="528fd7c7a8bb809f2046be12bd897541"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2023 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script><script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>