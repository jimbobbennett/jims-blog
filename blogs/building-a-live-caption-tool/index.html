<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Building a live caption tool - part 1"><meta property="og:description" content="Learn how to build a live captioner using Python and the Azure Cognitive Services"><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/building-a-live-caption-tool/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/building-a-live-caption-tool/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2019-07-02T11:39:24+00:00"><meta property="article:modified_time" content="2019-07-02T11:39:24+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/building-a-live-caption-tool/banner.png"><meta name=twitter:title content="Building a live caption tool - part 1"><meta name=twitter:description content="Learn how to build a live captioner using Python and the Azure Cognitive Services"><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><link rel=alternate type=application/rss+xml href=https://jimbobbennett.dev/index.xml title=JimBobBennett><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Building a live caption tool - part 1 | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Building a live caption tool - part 1</h1><div class="text-left content">Jim Bennett
<small>|</small>
Jul 2, 2019</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>I&rsquo;ve started a <a href=https://twitch.tv/jimbobbennett>Twitch stream where I&rsquo;m learning Python</a> every Wednesday at 12pm UK time. One way I&rsquo;d like to make my stream more accessible is by having live captions whilst I&rsquo;m speaking.</p><p>What I need is a tool that will stream captions to something I can add to my OBS scenes, but also be customizable. A lot of off the shelf speech to text models are great, but I need something I can tune to my voice and accent, as well as any special words I am using such as technical tools and terms.</p><p>The <a href="https://azure.microsoft.com/services/cognitive-services/directory/speech/?WT.mc_id=livecaption-blog-jabenn">Azure Cognitive Services</a> have such a tool - as well as using a standard speech to text model, you can customize the model for your voice, accent, background noise and special words.</p><p>In this part, I&rsquo;ll show how to get started building a live captioner in Python. In the next part, I&rsquo;ll show how to customize the output.</p><h2 id=create-the-speech-resource>Create the speech resource</h2><p>To get started, you first need to create a Speech resource in Azure. You can do it from the Azure Portal by following <a href="https://portal.azure.com/?WT.mc_id=twitchcaptions-blog-jabenn#create/Microsoft.CognitiveServicesSpeechServices">this link</a>. There is a free tier which I&rsquo;m using - after all we all love free stuff!</p><blockquote><p>If you don&rsquo;t have an Azure account you can create a free account at <a href="?WT.mc_id=twitchcaptions-blog-jabenn">azure.microsoft.com/free</a> and get $200 of free credit for the first 30 days and a host of services free for a year. Students and academic faculty can sign up at <a href="https://azure.microsoft.com/free/students/?WT.mc_id=livecaption-blog-jabenn">azure.microsoft.com/free/students</a> and get $100 that lasts a year as well as 12 months of free services, and this can be renewed every year that you are a student.</p></blockquote><figure><img src=2019-07-02_11-21-40.png></figure><p>When the resource is created, note down the first part of the endpoint from the <strong>Overview</strong> tab. The endpoint will be something like <code>https://uksouth.api.cognitive.microsoft.com/sts/v1.0/issuetoken</code>, and the bit you want is the part before <code>api.microsoft.com</code>, so in my case <code>uksouth</code>. This will be the name of the region you created your resource in. You all also need to grab a key from the <strong>Keys</strong> tab.</p><p>Once you have your Speech resource the next step is to use it to create captions.</p><h2 id=create-a-captioner>Create a captioner</h2><p>Seeing as my stream is all about learning Python, I thought it would be fun to build the captioner in Python. All the Microsoft Cognitive Services have <a href="https://azure.microsoft.com/resources/samples/cognitive-services-python-sdk-samples/?WT.mc_id=livecaption-blog-jabenn">Python APIs</a> which makes them easy to use.</p><p>I launched VS Code (which has excellent Python support thanks to the <a href="https://code.visualstudio.com/docs/languages/python/?WT.mc_id=livecaption-blog-jabenn">Python extension</a>), and created a new Python project. The Speech SDK is available via <code>pip</code>, so I installed via the Terminal it using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pip install azure-cognitiveservices-speech
</span></span></code></pre></div><p>To recognize speech you need to create a <code>speechRecognizer</code>, telling it the details of your resource via a <code>speechConfig</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> azure.cognitiveservices.speech <span style=color:#66d9ef>as</span> speechsdk
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>speech_config <span style=color:#f92672>=</span> speechsdk<span style=color:#f92672>.</span>SpeechConfig(subscription<span style=color:#f92672>=</span>speech_key, region<span style=color:#f92672>=</span>service_region)
</span></span><span style=display:flex><span>speech_recognizer <span style=color:#f92672>=</span> speechsdk<span style=color:#f92672>.</span>SpeechRecognizer(speech_config<span style=color:#f92672>=</span>speech_config)
</span></span></code></pre></div><p>In the code above, replace <code>speech_key</code> with the key from the Speech resource, and replace <code>service_region</code> with the region name.</p><blockquote><p>This will create a speech recognizer using the default microphone. If you want to change the microphone you will need to know the device id and use this to create an <code>AudioConfig</code> object which is used to create the recognizer. You can read more about this in <a href="https://docs.microsoft.com/azure/cognitive-services/speech-service/how-to-select-audio-input-devices/?WT.mc_id=twitchcaptions-blog-jabenn">the docs</a>.</p></blockquote><p>The speech recognizer can be run as a one off and listen for a single block of speech until a break is found, or it can run continuously providing a constant stream of text via events. To detect continuously, an event needs to be wired up to collect the text.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recognizing</span>(args):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Do something</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>speech_recognizer<span style=color:#f92672>.</span>recognizing<span style=color:#f92672>.</span>connect(recognizing)
</span></span><span style=display:flex><span>speech_recognizer<span style=color:#f92672>.</span>start_continuous_recognition()
</span></span></code></pre></div><p>In the above code, the <code>recognizing</code> event is fired every time some text is recognized. This event is fired multiple times for the same set of words, building up the text over time as the model refines the output. After a break it will reset and send new text.</p><p>The <code>args</code> parameter is a <code>SpeechRecognitionEventArgs</code> instance with a property called <code>result</code> that contains the result of the recognition. This result has a property called <code>text</code> with the recognized text.</p><p>For example, if you run this and say &ldquo;Hello and welcome to the speech captioner&rdquo;, this event will be called probably 7 times:</p><pre tabindex=0><code>hello
hello and
hello and welcome
hello and welcome to
hello and welcome to the
hello and welcome to the speech
hello and welcome to the speech captioner
</code></pre><p>If you then pause and say &ldquo;This works&rdquo; it will be called 2 more times, with just the new words.</p><pre tabindex=0><code>this
this works
</code></pre><p>The text is refined as the words are analyzed, so the text can change over time. For example if you say &ldquo;This is a live caption test&rdquo;, you may get back:</p><pre tabindex=0><code>this
this is
this is alive
this is a live caption
this is a live caption text
</code></pre><p>Notice in the third result there is the word &ldquo;alive&rdquo;, which gets split into &ldquo;a live&rdquo; as more context is understood by the model.</p><p>The model doesn&rsquo;t understand sentences, and in reality humans rarely speak in coherent sentences with a structure that is easy for the model to break up, hence why you won&rsquo;t see full stops or capital letters.</p><p>The <code>start_continuous_recognition</code> call will run the recognition in the background, so the app will need a way to keep running, such as a looping sleep or an app loop using a GUI framework like Tkinter.</p><p>I&rsquo;ve created a GUI app using Tkinter using this code. My app will put a semi-opaque window at the bottom of the screen that has a live stream of the captions in a label. The label is updated with the text from the <code>recognizing</code> event, so will be updated as I speak, then cleared down after each block of text ends and a new one begins.</p><p>You can find it on <a href=https://github.com/jimbobbennett/TwitchCaptioner>GitHub</a>, to use it add your key and region to the <em>config.py</em> file, install the <code>pip</code> packages from the <em>requirements.txt</em> file and run <em>captioner.py</em> through Python.</p><p>In the next part, I&rsquo;ll show how to customize the model to my voice and terms I use.</p></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents><ul><li><a href=#create-the-speech-resource>Create the speech resource</a></li><li><a href=#create-a-captioner>Create a captioner</a></li></ul></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/technology target=_blank>technology</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/python target=_blank>Python</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/azure target=_blank>azure</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/speech target=_blank>speech</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/building-a-live-caption-tool/",this.page.identifier="b3e0c531f874362e76990ab0e4faf3ff"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2022 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script>
<script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>