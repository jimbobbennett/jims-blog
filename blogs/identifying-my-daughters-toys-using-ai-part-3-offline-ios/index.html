<!doctype html><html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Identifying my daughters toys using AI - Part 3, using the models offline in iOS"><meta property="og:description" content="In the first part of this series I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
In the second part I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service."><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2018-01-17T17:13:02+00:00"><meta property="article:modified_time" content="2018-01-17T17:13:02+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/banner.png"><meta name=twitter:title content="Identifying my daughters toys using AI - Part 3, using the models offline in iOS"><meta name=twitter:description content="In the first part of this series I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
In the second part I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service."><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><link rel=alternate type=application/rss+xml href=https://jimbobbennett.dev/index.xml title=JimBobBennett><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><script data-goatcounter=https://jimbobbennett.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Identifying my daughters toys using AI - Part 3, using the models offline in iOS | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/podcasts title=Podcasts>Podcasts</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Identifying my daughters toys using AI - Part 3, using the models offline in iOS</h1><div class="text-left content"><a href=https://linkedin.com/in/jimbobbennett>Jim Bennett
</a><small>|</small>
Jan 17, 2018</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai/>first part of this series</a> I used the <a href="http://customvision.ai/?wt.mc_id=toyidentifier-blog-jabenn">Azure Custom Vision service</a> to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.</p><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/>second part</a> I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service.</p><p>The limitation with this NuGet package is that it requires internet access. Not such a huge problem for classifying single images with fast, cheap data contracts, but not so good in countries where data is expensive, or when classifying a live video feed.</p><p>Identifying toys is fun, and is a great project I can share with my daughter, but there are a lot or serious uses for this kind of technology, especially in medical imaging.</p><p>Imagine an app where you can run your phone over moles on your skin and have it tell you in real-time if you are at risk of skin cancer. You&rsquo;d want this to run on device for a couple of reasons. Firstly for speed, uploading a lot of images is slow. Secondly, and more importantly, this kind of technology is incredibly useful for third-world countries who have little in the way of education around health issues, or access to medical screening services. These countries also have little or no, or incredibly expensive internet access, especially away from big cities where people are more likely to be lacking medical screening. If you could take a phone on the road with these models running locally you could easily screen whole villages in a short space of time.</p><p>This kind of app is not a sci-fi dream, but is a real thing. You can see an example iOS app and download actual trained models for identifying skin cancer at <a href=https://github.com/Azure/ai-toolkit-iot-edge/tree/master/Skin%20cancer%20detection>https://github.com/Azure/ai-toolkit-iot-edge/tree/master/Skin%20cancer%20detection</a>.</p><p>These models run on device using CoreML on iOS and TensorFlow on Android. In this post we&rsquo;ll look at CoreML, in the next post we&rsquo;ll look at TensorFlow.</p><div class=image-div style=max-width:128px><p><img src=core-ml-128x128_2x.png alt="CoreML logo"></p></div><h4 id=running-models-on-coreml>Running models on CoreML</h4><p><a href=https://developer.apple.com/documentation/coreml>CoreML</a> was introduced in iOS 11 and macOS High Sierra, and is a set of APIs for running models on device instead of up in the cloud. The models used are compressed and optimized to run on device GPUs, and they run pretty quickly. CoreML can run all sorts of models, and has a specialized API designed for running image classification models that we can take advantage of.</p><div class=image-div style=max-width:400px><p><img src=https://docs-assets.developer.apple.com/published/72e22672fd/c35ebf2d-ee94-4448-8fae-16420e7cc4ed.png alt="CoreML runs models inside your app"></p></div><p>To download the CoreML model, head to the <strong>Performance</strong> tab in the Custom Vision portal, selected the latest iteration from the list on the left (we&rsquo;ll cover iterations in a future post), and click the <strong>Export</strong> link at the top. Select <strong>iOS 11 (CoreML)</strong> then click <strong>Download</strong>. This will download a <code>.mlmodel</code> file. Before you can use this model, you will need to compile it - either in advance or at run time.</p><p>To compile in advance on a Mac using the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>xcrun coremlcompiler compile &lt;model_name&gt;.mlmodel &lt;compiled_model_name&gt;.mlmodelc
</span></span></code></pre></div><p>The <code>model_name</code> will be a GUID, so set the <code>compiled_model_name</code> to be something more user friendly. Once this is compiled you will have a folder called <code>compiled_model_name.mlmodelc</code> containing some model files. You will need to add this entire folder to your <code>Resources</code> folder in your iOS app, so drag this into Visual Studio.</p><div class=image-div style=max-width:200px><p><img src=2018-01-17_13-16-07.png alt="The Resources folder with a compiled model in it"></p></div><br><p>You can also compile it at runtime. This is useful especially if you want to download new models without updating your app. If you want to compile at runtime, just add the <code>&lt;model_name>.mlmodel</code> file to your resources folder, renaming the model to something easier to read than the default Guid.</p><p>To run CoreML models you need three components:</p><ul><li>The model</li><li>A request for the model with a callback</li><li>A request handler to issue the request</li></ul><p><strong>The model</strong></p><p>Once we have the model as a resource, we need to load it into a CoreML model, then convert it to a vision model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>using</span> CoreML;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> Vision;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> assetPath = GetModel();
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> mlModel = MLModel.Create(assetPath, <span style=color:#66d9ef>out</span> NSError err);
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> model = VNCoreMLModel.FromMLModel(mlModel, <span style=color:#66d9ef>out</span> err);
</span></span></code></pre></div><p>The <code>GetModel</code> method varies depending on if the model is compiled or not. For compiled models use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>private</span> NSUrl GetModel()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> NSBundle.MainBundle.GetUrlForResource(<span style=color:#e6db74>&#34;&lt;compiled_model_name&gt;&#34;</span>, <span style=color:#e6db74>&#34;mlmodelc&#34;</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>To compile your model on device use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>private</span> NSUrl GetModel()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>var</span> url = NSBundle.MainBundle.GetUrlForResource(<span style=color:#e6db74>&#34;&lt;model_name&gt;&#34;</span>, <span style=color:#e6db74>&#34;mlmodel&#34;</span>)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> MLModel.CompileModel(url, our NSError err);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><blockquote><p>In a real app you should check the NSError out variables for any errors</p></blockquote><p>This code creates a <code>MLModel</code> from a URL to a resource, then converts this to a <code>VNCoreMLModel</code> - a &lsquo;Vision&rsquo; CoreML model for image classification constructed using the custom vision model. The <code>Vision</code> namespace also contains some built in models for bar code recognition and facial recognition, amongst others.</p><p>If you want more information on this class, the docs for VNCoreMLModel are <a href="https://developer.xamarin.com/api/type/Vision.VNCoreMLModel/?wt.mc_id=toyidentifier-blog-jabenn">here</a>.</p><p><strong>The request</strong></p><p>The CoreML request wraps our model and provides a callback. Running these models is asynchronous - once you start it, it will run on the GPU in a background thread and invoke the callback once done.</p><p>The code to create the request is:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> request = <span style=color:#66d9ef>new</span> VNCoreMLRequest(_model, (response, e) =&gt; {});
</span></span></code></pre></div><p>The second parameter is the callback, with the <code>response</code> parameter containing the results from the model, and the <code>e</code> parameter being an NSError instance in case of an error.</p><p>If you want more information on this class, the docs for VNCoreMLRequest are <a href="https://developer.xamarin.com/api/type/Vision.VNCoreMLRequest/?wt.mc_id=toyidentifier-blog-jabenn">here</a>.</p><p><strong>The request handler</strong></p><p>Although this model is for image classification, it actually doesn&rsquo;t understand what an image is - it works on raw binary data only. The custom vision models expects this binary data to be in a specific format, namely a 227x227 CoreVision pixel buffer using 32-bit ARGB, whatever that is! Luckily it&rsquo;s easy to create one of these using code I found on <a href=https://github.com/xamarin/ios-samples/blob/master/ios11/CoreMLImageRecognition/CoreMLImageRecognition/ClassExtensions/UIImageExtensions.cs>one of the Xamarin iOS CoreML samples</a>. Follow that link for an extension method that converts from a <code>UIImage</code> to a pixel buffer.</p><p>Once we have the pixel buffer, we create a request handler for our buffer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> buffer = source.ToCVPixelBuffer(_targetImageSize);
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> requestHandler = <span style=color:#66d9ef>new</span> VNImageRequestHandler(buffer, <span style=color:#66d9ef>new</span> NSDictionary());
</span></span></code></pre></div><p>We can then invoke the request handler with our request:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span>requestHandler.Perform(<span style=color:#66d9ef>new</span>[] { request }, <span style=color:#66d9ef>out</span> NSError error);
</span></span></code></pre></div><p>This handler can be used to run multiple requests at once on the same pixel buffer, just pass all the requests you want to run as items in the array in the first parameter. This is useful if you wanted to screen for multiple conditions in a medical imaging app, with multiple models for each condition run against the same image.</p><p>If you want more information on this class, the docs for VNImageRequestHandler are <a href="https://developer.xamarin.com/api/type/Vision.VNImageRequestHandler/?wt.mc_id=toyidentifier-blog-jabenn">here</a>.</p><p><strong>Handling the response</strong></p><p>Once the model has been run, it will call the callback you passed to the request, passing in the response (or an error if it failed). This response has a method on it, <code>GetResults&lt;T>()</code> that returns an array of results of whatever type the model spits out. For vision models, the results are of type <code>VNClassificationObservation</code>, so inside our callback we would do:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> results = response.GetResults&lt;VNClassificationObservation&gt;();
</span></span></code></pre></div><p><code>VNClassificationObservation</code> has two fields that we are interested in:</p><ul><li><code>Identifier</code> - this is the tag we set in our model</li><li><code>Confidence</code> - this is the probability that our image matches our tag, from 0 to 1 with 1 being 100% confidence.</li></ul><p>These values match up to the <code>Tag</code> and <code>Probability</code> fields from the <code>ImageTagPredictionModel</code> class in the Custom Vision NuGet package. You can read more on the VNClassificationObservation class <a href="https://developer.xamarin.com/api/type/Vision.VNClassificationObservation/?WT.mc_id=toyidentifier-blog-jabenn">here</a>.</p><p>For an example of this code in action, check out my <a href=https://github.com/jimbobbennett/Xam.Plugins.OnDeviceCustomVision>Xamarin.PlugIn.OnDeviceCustomVision</a> NuGet package. This wraps the code above, along with an Android implementation using TensorFlow (which we&rsquo;ll look at in the next post) in an easy to use Xamarin Plugin.</p><blockquote><p>CoreML is only supported on iOS 11 and above - so don&rsquo;t forget to set your deployment target to iOS 11 in your <code>info.plist</code></p></blockquote><p>You can read more on exporting and using models <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/export-your-model?wt.mc_id=toyidentifier-blog-jabenn">here</a>. In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-4-offline-android/>next post</a> in this series we&rsquo;ll see how to run models on device on Android using TensorFlow.</p></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/technology target=_blank>technology</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin target=_blank>xamarin</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ai target=_blank>AI</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/coreml target=_blank>CoreML</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/azure target=_blank>azure</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/custom-vision target=_blank>custom vision</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/porg target=_blank>Porg</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/image-classifier target=_blank>Image classifier</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ios target=_blank>iOS</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/",this.page.identifier="1df3fd9279a61fd0a081b6c38ba0b0a5"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2024 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script><script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>