<!doctype html><html><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Save your users from typing with AI 🤖"><meta property="og:description" content="I&rsquo;ve been playing a lot with the Azure Custom Vision Service - a great service to build image classification and object detection models with just a few images. To make it easier to test my models, I wanted to build a mobile app that connects to a custom vision project and uses that to classify or detect objects in images captured using the camera.
The app is pretty simple, it&rsquo;s a Xamarin app that uses a camera plugin and the Custom Vision SDKs."><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/save-your-users-from-typing-with-ai/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/save-your-users-from-typing-with-ai/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2019-07-21T21:52:08+00:00"><meta property="article:modified_time" content="2019-07-21T21:52:08+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/save-your-users-from-typing-with-ai/banner.png"><meta name=twitter:title content="Save your users from typing with AI 🤖"><meta name=twitter:description content="I&rsquo;ve been playing a lot with the Azure Custom Vision Service - a great service to build image classification and object detection models with just a few images. To make it easier to test my models, I wanted to build a mobile app that connects to a custom vision project and uses that to classify or detect objects in images captured using the camera.
The app is pretty simple, it&rsquo;s a Xamarin app that uses a camera plugin and the Custom Vision SDKs."><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><link rel=alternate type=application/rss+xml href=https://jimbobbennett.dev//index.xml title=JimBobBennett><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><script data-goatcounter=https://jimbobbennett.goatcounter.com/count async src=//gc.zgo.at/count.js></script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Save your users from typing with AI 🤖 | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/podcasts title=Podcasts>Podcasts</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Save your users from typing with AI 🤖</h1><div class="text-left content"><a href=https://linkedin.com/in/jimbobbennett>Jim Bennett
</a><small>|</small>
Jul 21, 2019</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>I&rsquo;ve been playing a lot with the <a href="https://customvision.ai/?WT.mc_id=textrecogniser-blog-jabenn">Azure Custom Vision Service</a> - a great service to build image classification and object detection models with just a few images. To make it easier to test my models, I wanted to build a mobile app that connects to a custom vision project and uses that to classify or detect objects in images captured using the camera.</p><figure><img src=/blogs/save-your-users-from-typing-with-ai/IMG_1383-1.PNG></figure><p>The app is pretty simple, it&rsquo;s a <a href="https://dotnet.microsoft.com/apps/xamarin/?WT.mc_id=textrecogniser-blog-jabenn">Xamarin</a> app that uses a camera plugin and the Custom Vision SDKs. There is a problem with it&mldr; Configuration.</p><p>To use a model you need four pieces of information:</p><ul><li>Prediction Key - a unique API key for you to use to classify or detect objects</li><li>Endpoint - these services can be run in different Azure regions, so you need the endpoint to show which region it is in</li><li>Project Id - you can have multiple projects, and each one is identified by a GUID</li><li>Publish name - each model has multiple iterations, and you publish the ones you want to be made available with a name</li></ul><p>To use my model, I need to configure my app with all for of these pieces of information. I didn&rsquo;t want to hard code them - I don&rsquo;t want to have to rebuild my app every time I change project or iteration. Ideally I should be able to set these at runtime and store them between sessions.</p><p>I started off with a simple settings screen to type these 4 values in. I made a <strong><strong>LOT</strong></strong> of mistakes doing this as the values are long and complex. I needed to make my app more usable and find a way to avoid errors&mldr;</p><figure><img src=/blogs/save-your-users-from-typing-with-ai/IMG_91CBC50D535B-1-1.jpeg></figure><p>The solution was to use OCR - optical character recognition, using AI to convert text in an image to a string value.</p><p>Microsoft has a text recognition service available as part of the <a href="https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=textrecogniser-blog-jabenn">Azure Cognitive Services</a>. This service takes an image, then detects text in it and returns that text.</p><h2 id=getting-started>Getting started</h2><p>To get started you need either an Azure account. You can sign up for free <a href="https://azure.microsoft.com/free/ai/?WT.mc_id=textrecogniser-blog-jabenn">here</a> if you don&rsquo;t already have an account.</p><blockquote><p>You can access a free 7 day guest account without signup from <a href="https://azure.microsoft.com/services/cognitive-services/computer-vision/?WT.mc_id=textrecogniser-blog-jabenn">here</a>.</p></blockquote><p>Once you are signed up, head to the <a href="https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesComputerVision/?WT.mc_id=textrecogniser-blog-jabenn">Azure Portal</a> to create a computer vision resource.</p><p>When you create this resource, you select what pricing tier you want - and there is a nice free tier allowing up to 20 requests per minute which is perfect for my app. Once created, you will need to grab the <em>Endpoint</em> from the <em>Overview</em> tab in the portal, and one of the keys from the <em>Keys</em> tab.</p><h2 id=using-the-service-from-code>Using the service from code</h2><p>There are a whole stack of SDKs available for Azure, covering the most popular languages and technologies, as well as a REST API for languages that SDKs are not available for. My app is a Xamarin app, so I used the .NET SDKs available on <a href="https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.ComputerVision/?WT.mc_id=textrecogniser-blog-jabenn">NuGet</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Install-Package Microsoft.Azure.CognitiveServices.Vision.ComputerVision -Version 5.0.0
</span></span></code></pre></div><p>Once the package was added, I just needed to take a picture and send it for processing. Taking pictures with a Xamarin app is not that complicated thanks to the <a href=https://github.com/jamesmontemagno/MediaPlugin>Media Plugin</a>, and I used this to take an image and give me a stream.</p><p>Once I had the image, I sent it to the <a href="https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-recognizing-text#read-api?WT.mc_id=textrecogniser-blog-jabenn">Read API</a>. This API takes an image and returns a token immediately whilst processing the image offline. You then call another API to check the status, and once the status comes back as processed you can grab the text.</p><h3 id=create-the-computer-vision-client>Create the computer vision client</h3><p>Before you can process an image, you need an instance of the <code>ComputerVisionClient</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> creds = <span style=color:#66d9ef>new</span> ApiKeyServiceClientCredentials(&lt;Your Key&gt;);
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> computerVision = <span style=color:#66d9ef>new</span> ComputerVisionClient(creds)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  Endpoint = &lt;Your Endpoint&gt;
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>In the code above, <code>&lt;Your Key></code> would be replaced with your API key for the computer vision service, and <code>&lt;Your Endpoint></code> would be replaced with the services endpoint.</p><h3 id=start-the-read-request>Start the read request</h3><p>To start using the Read API, you send the image to call to one of the <code>BatchRead</code> functions. There are several of these, depending on if you want to process a file or a stream. In my case I&rsquo;m using a stream.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> results = <span style=color:#66d9ef>await</span> computerVision.BatchReadFileInStreamAsync(photo.GetStream());
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> loc = results.OperationLocation;
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> operationId = loc.Substring(loc.Length - <span style=color:#ae81ff>36</span>);
</span></span></code></pre></div><p>The call to <code>BatchReadFileInStreamAsync</code> returns an intermediate result, and this has an <code>OperationLocation</code> - a token for the read operation. This is a long URL with the token on the end, so we need to extract the last 36 characters to get the token.</p><p>The result is an intermediate result, you will need to wait for the processing to finish</p><h3 id=wait-for-the-request-to-finish>Wait for the request to finish</h3><p>The request will take some time to run, as in a few milliseconds as opposed to minutes. The result of the batch call contains the status, and if this status is not complete then it can be retrieved continuously until it is complete, waiting between retries.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>while</span> (result.Status == TextOperationStatusCodes.Running ||     
</span></span><span style=display:flex><span>       result.Status == TextOperationStatusCodes.NotStarted)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>await</span> Task.Delay(<span style=color:#ae81ff>100</span>);
</span></span><span style=display:flex><span>  result = <span style=color:#66d9ef>await</span> computerVision.GetReadOperationResultAsync(operationId);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This code loops whilst the status is <code>NotStarted</code> or <code>Running</code>, pausing for 100ms then requesting the new status. Ideally you would also limit the retries and check for a <code>Failed</code> status.</p><h3 id=get-the-text>Get the text</h3><p>Once the status is <code>Succeeded</code>, the results will be available. The results come back as a list of <code>TextRecognitionResult</code>, each one referring to an area of text in the image. Each <code>TextRecognitionResult</code> contains one or more lines of text in the <code>Lines</code> property, and each line of text is accompanied by extra details such as the bounding box of the text. The lines are from top to bottom, so the topmost line of text would be the first entry in the <code>Lines</code> property, the next line down the next entry and so on.For my purposes, the settings page on the custom vision service has the data I need in a vertical list with the title of the field above the value, such as for the <strong>Project Id</strong> shown below.</p><figure><img src=/blogs/save-your-users-from-typing-with-ai/ProjectId.png></figure><p>In this case if I photographed these values and detected the text I would have one line containing the text <code>"Project Id"</code> with the next line being the GUID for the project Id.</p><figure><img src=/blogs/save-your-users-from-typing-with-ai/ProjectIdImmediate.png></figure><p>I can now extract this value and use it in my app.</p><h3 id=done>Done</h3><p>Using the text recognition service from the Azure Cognitive Services means you will never have to ask your users to type in complicated data ever again!</p><hr><p>You can find the code for my app on GitHub here: <a href=https://github.com/jimbobbennett/ObjectDetector>github.com/jimbobbennett/ObjectDetector</a>. All the code that uses the cognitive services is in the <code>SettingsViewModel</code>.</p></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents><ul><li><a href=#getting-started>Getting started</a></li><li><a href=#using-the-service-from-code>Using the service from code</a><ul><li><a href=#create-the-computer-vision-client>Create the computer vision client</a></li><li><a href=#start-the-read-request>Start the read request</a></li><li><a href=#wait-for-the-request-to-finish>Wait for the request to finish</a></li><li><a href=#get-the-text>Get the text</a></li><li><a href=#done>Done</a></li></ul></li></ul></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/technology target=_blank>Technology</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin target=_blank>xamarin</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin.forms target=_blank>xamarin.forms</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ai target=_blank>AI</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ocr target=_blank>OCR</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/text-recognition target=_blank>Text recognition</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/save-your-users-from-typing-with-ai/",this.page.identifier="f734eb0f6be7cfd0f03851442531bbd1"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2024 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script><script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>