<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=icon href=/fav.png type=image/png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora&family=Muli:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto&family=Muli:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><meta property="og:title" content="Identifying my daughters toys using AI - Part 4, using the models offline on Android"><meta property="og:description" content="In the first part of this series I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
In the second part I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service."><meta property="og:type" content="article"><meta property="og:url" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-4-offline-android/"><meta property="og:image" content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-4-offline-android/banner.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2018-01-24T17:43:53+00:00"><meta property="article:modified_time" content="2018-01-24T17:43:53+00:00"><meta property="og:site_name" content="JimBobBennett"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-4-offline-android/banner.png"><meta name=twitter:title content="Identifying my daughters toys using AI - Part 4, using the models offline on Android"><meta name=twitter:description content="In the first part of this series I used the Azure Custom Vision service to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.
In the second part I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service."><meta name=twitter:site content="@jimbobbennett"><meta name=twitter:creator content="@jimbobbennett"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css integrity=sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3 crossorigin=anonymous><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><link rel="shortcut icon" type=image/png href=/fav.png><link rel="shortcut icon" sizes=192x192 href=/fav.png><link rel=apple-touch-icon href=/fav.png><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","dctc2ydykv")</script><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--background-color:#000;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--background-color-dark:#000000;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{background-color:#000;font-size:1rem;font-weight:400;line-height:1.5;text-align:left}</style><meta name=description content><link rel=stylesheet href=/css/index.css><link rel=stylesheet href=/css/single.css><link rel=stylesheet href=/css/projects.css media=all><script defer src=/fontawesome-5/all-5.15.4.js></script><title>Identifying my daughters toys using AI - Part 4, using the models offline on Android | JimBobBennett</title></head><body class=light onload=loading()><header><nav class="pt-3 navbar navbar-expand-lg"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/fav.png width=30 height=30 class="d-inline-block align-top">
JimBobBennett</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation"><svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text"><a class=nav-link href=/ aria-label=home>Home</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Recent Highlights</a></li><li class="nav-item navbar-text"><a class=nav-link href=/blogs title="Blog posts">Blog</a></li><li class="nav-item navbar-text"><a class=nav-link href=/videos title=Videos>Videos</a></li><li class="nav-item navbar-text"><a class=nav-link href=/livestreams title=Livestreams>Livestreams</a></li><li class="nav-item navbar-text"><a class=nav-link href=/conferences title=Conferences>Conferences</a></li><li class="nav-item navbar-text"><a class=nav-link href=/resume title=Resume>Resume</a></li></ul></div></div></nav></header><div id=content><section id=projects><div class="container pt-5" id=list-page><div class="row justify-content-center px-3 px-md-5"><h1 class="text-left pb-2 content">Identifying my daughters toys using AI - Part 4, using the models offline on Android</h1><div class="text-left content">Jim Bennett
<small>|</small>
Jan 24, 2018</div></div></div></section><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><article class="page-content p-2"><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai/>first part of this series</a> I used the <a href="http://customvision.ai/?wt.mc_id=toyidentifier-blog-jabenn">Azure Custom Vision service</a> to create an image classifier to allow me to easily identify my daughters cuddly toys. Once created I tested it by uploading an image and seeing what tags the classifier found for the image.</p><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-2-using-the-model/>second part</a> I accessed this model from a Xamarin app, so that I could use the camera to take a photo to run through the classifier using a NuGet package that talks to the Custom Vision service.</p><p>In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-3-offline-ios/>third part</a> I showed how to download this model for iOS and run it locally, on device, using CoreML.</p><p>In this part we&rsquo;re going to switch OS and run these models on Android. Like with iOS, Android offers some APIs to run AI models on device, taking advantage of the hardware. Unlike iOS this is not an API that is baked into the OS, instead it is a separate library you can add to your project and run. The upside of this is it has more OS support, working all the way back to API 21.</p><div class=image-div style=max-width:128px><p><img src=https://www.tensorflow.org/_static/images/tensorflow/logo.png alt="TensorFlow logo"></p></div><p>The library in question is called <a href=https://www.tensorflow.org>TensorFlow</a>, and is actually an open source AI library that came from Google that can run on pretty much any platform and is accessible from most languages. When running on device you can think of it as similar to CoreML on iOS, a generic way of running all sorts of machine learning models including the ones generated by the custom vision service.</p><h4 id=running-models-with-tensorflow>Running models with TensorFlow</h4><p>For mobile apps we can use the Android bindings for the TensorFlow library. Like with CoreML these bindings contain a full API for running all sorts of models, as well as an easy API surface that we can use to do image classification - similar to the CoreML Vision APIs.</p><p>To download the TensorFlow model, head to the <strong>Performance</strong> tab in the Custom Vision portal, selected the latest iteration from the list on the left (we&rsquo;ll cover iterations in a future post), and click the <strong>Export</strong> link at the top. Select <strong>Android (TensorFlow)</strong> then click <strong>Download</strong>. This will download a zip file containing 2 files:</p><ul><li><code>model.pb</code> - This is the actual TensorFlow model</li><li><code>labels.txt</code> - The labels for the tags</li></ul><p>Once these files have been downloaded you will need to add them to the <code>Assets</code> folder in your Android app.</p><p><strong>Importing the Android TensorFlow bindings</strong></p><p>The TensorFlow Android bindings consist of a native library with a Jar binding. To use these from a Xamarin app we need to add Xamarin bindings, and this has been done for us already by Larry O&rsquo;Brien from Xamarin and is available on GitHub here: <a href=https://github.com/lobrien/TensorFlow.Xamarin.Android>https://github.com/lobrien/TensorFlow.Xamarin.Android</a>. This binding is not available on NuGet yet, so you will need to clone this repo and compile it yourself. Once you&rsquo;ve compiled it you will need to add the resulting <code>TensorFlowXamain.dll</code> to your Android app.</p><p><strong>Creating the model</strong></p><p>The TensorFlow binding includes a class called <code>TensorFlowInferenceInterface</code> which can be used to easily run image classification models.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>using</span> Org.Tensorflow.Contrib.Android;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> assets = Android.App.Application.Context.Assets;
</span></span><span style=display:flex><span><span style=color:#66d9ef>var</span> inferenceInterface = <span style=color:#66d9ef>new</span> TensorFlowInferenceInterface(assets, <span style=color:#e6db74>&#34;model.pb&#34;</span>);
</span></span></code></pre></div><p>This will then load the <code>model.pb</code> model file from the asset catalog into a model.</p><p>Once we have the model we need to feed it some data, run it, then extract and interpret the output.</p><p><strong>Feeding the model</strong></p><div class=image-div style=max-width:250px><p><img src=cec2a9217e2ee21f01abc6ae2d41b910264bd545610339ad2dbd06631e163d45.jpg alt="Feed me Seymour"></p></div><p>Just like with CoreML the model doesn&rsquo;t understand images as such, instead it needs binary data in the same format - a 227x227 sized array of 32-bit ARGB values. Again this is sort of pretty easy to create from an Android <code>Bitmap</code>, we just need to convert the bitmap to one of the right size and color space. Once it&rsquo;s in the right color space we need to average out the colors - models work better when the average of all the color values is 0 (neural networks work better when the average of all inputs is 0, thanks to the ever awesome <a href=https://twitter.com/praeclarum>Frank Krueger</a> for teaching me this). Different domains are trained in different ways, so will need different adjustments.</p><p>You can see some Java sample code for this at <a href=https://github.com/Azure-Samples/cognitive-services-android-customvision-sample>https://github.com/Azure-Samples/cognitive-services-android-customvision-sample</a>, with details on the adjustments you need to make to the image bytes detailed in the ReadMe.</p><p>Luckily I&rsquo;ve done the hard work converting it to Xamarin for you and it&rsquo;s on GitHub <a href=https://github.com/jimbobbennett/Xam.Plugins.OnDeviceCustomVision/blob/master/Xam.Plugins.OnDeviceCustomVision.Droid/ImageExtensions.cs>here</a>.</p><p>Once you have the binary data, you pass it to the TensorFlow inference interface as named data, called &ldquo;Placeholder&rdquo;. This name is required by the models exported from the Custom Vision service:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span>inferenceInterface.Feed(<span style=color:#e6db74>&#34;Placeholder&#34;</span>, floatValues, <span style=color:#ae81ff>1</span>, InputSize, InputSize, <span style=color:#ae81ff>3</span>);
</span></span></code></pre></div><p>The additional parameters provide details on the float buffer - I honestly don&rsquo;t know what the &ldquo;1&rdquo; parameter is for, the next three are the size of the buffer (in our case 227 for both) and the number of floats per pixel in the buffer - 3 for R, G and B.</p><p><strong>Running the model</strong></p><p>Once the model has been fed, it needs to be run for a list of outputs - models can produce multiple outputs so we need to run it for all the outputs we need. In our case the only output we need is called &ldquo;loss&rdquo;:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span>inferenceInterface.Run(<span style=color:#66d9ef>new</span>[] { <span style=color:#e6db74>&#34;loss&#34;</span> });
</span></span></code></pre></div><p><strong>Getting the outputs</strong></p><p>Once the model has run, we can extract the output that we are interested in - in our case the &ldquo;loss&rdquo; output. This comes back as an array of floats containing one entry per tag, with the values representing the probability of the image matching that tag as a value between 0 and 1, 1 being 100% probability. We have to pre-create this array before passing it in - so how do we know how big it is, and how do we know which tag value is which?</p><p>The answer comes from the <code>labels.txt</code> file that was downloaded along with the <code>model.pb</code> file. This file contains a list of tags, one per line. Add this file to your apps assets, then load it using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> assets = Android.App.Application.Context.Assets;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> (<span style=color:#66d9ef>var</span> sr = <span style=color:#66d9ef>new</span> StreamReader(assets.Open(<span style=color:#e6db74>&#34;labels.txt&#34;</span>)))
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>var</span> content = sr.ReadToEnd();
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>var</span> labels = content.Split(<span style=color:#e6db74>&#39;\n&#39;</span>)
</span></span><span style=display:flex><span>                      .Select(s =&gt; s.Trim())
</span></span><span style=display:flex><span>                      .Where(s =&gt; !<span style=color:#66d9ef>string</span>.IsNullOrEmpty(s))
</span></span><span style=display:flex><span>                      .ToList();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This will give you a list of labels - the file contains an empty line on the end by default, so remember to trim whitespace and remove any empty lines. You can then create a float array of the same size and put the output in there:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cs data-lang=cs><span style=display:flex><span><span style=color:#66d9ef>var</span> outputs = <span style=color:#66d9ef>new</span> <span style=color:#66d9ef>float</span>[labels.Count];
</span></span><span style=display:flex><span>inferenceInterface.Fetch(<span style=color:#e6db74>&#34;loss&#34;</span>, outputs);
</span></span></code></pre></div><p>The float values map index for index with the label - so if <code>labels[0]</code> was <code>foo</code> and <code>labels[1]</code> was <code>bar</code>, <code>outputs[0]</code> would be the probability of the image being <code>foo</code>, and <code>outputs[1]</code> would be the probability of the image being <code>bar</code>.</p><blockquote><p>TensorFlow is only supported on API 21 and above - so don&rsquo;t forget to set your minimum supported version to 21 in your <code>application.manifest</code></p></blockquote><p>You can read more on exporting and using models <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/export-your-model?wt.mc_id=toyidentifier-blog-jabenn">here</a>. In the <a href=/blogs/identifying-my-daughters-toys-using-ai-part-5-plugin-for-on-device-models/>next post</a> in this series we&rsquo;ll look at the plugin NuGet package I&rsquo;ve created to make it easy to use these models from a cross platform app.</p></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div class=sticky-sidebar><aside class=toc><h5>Table Of Contents</h5><div class=toc-content><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></aside><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin.android target=_blank>xamarin.android</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/android target=_blank>Android</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/xamarin target=_blank>xamarin</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/tensorflow target=_blank>Tensorflow</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/ai target=_blank>AI</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/custom-vision target=_blank>custom vision</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/cognitive-services target=_blank>cognitive services</a></li><li class=list-inline-item><a href=https://jimbobbennett.dev/tags/azure target=_blank>azure</a></li></ul></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://jimbobbennett.dev/blogs/identifying-my-daughters-toys-using-ai-part-4-offline-android/",this.page.identifier="3f67ad236f6aef7cc2364abf35ed7481"};(function(){if(window.location.hostname=="localhost")return;var e=document,t=e.createElement("script");t.src="https://jimbobbennett.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></div><footer><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center">&copy; 2022 All Rights Reserved</div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js integrity=sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13 crossorigin=anonymous></script>
<script>document.body.className.includes("light")&&(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))</script><script>let loadingIcons;function loading(){myVar=setTimeout(showPage,100)}function showPage(){try{document.getElementById("loading-icons").style.display="block"}catch{}}</script><script>function createCopyButton(e,t){const n=document.createElement("button");n.className="copy-code-button",n.type="button",n.innerText="Copy",n.addEventListener("click",()=>copyCodeToClipboard(n,e,t)),addCopyButtonToDom(n,e)}async function copyCodeToClipboard(e,t,n){const s=t.querySelector("pre > code").innerText;try{n.writeText(s)}finally{codeWasCopied(e)}}function codeWasCopied(e){e.blur(),e.innerText="Copied!",setTimeout(function(){e.innerText="Copy"},2e3)}function addCopyButtonToDom(e,t){t.insertBefore(e,t.firstChild);const n=document.createElement("div");n.className="highlight-wrapper",t.parentNode.insertBefore(n,t),n.appendChild(t)}if(navigator&&navigator.clipboard)document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,navigator.clipboard));else{var script=document.createElement("script");script.src="https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js",script.integrity="sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=",script.crossOrigin="anonymous",script.onload=function(){addCopyButtons(clipboard)},document.querySelectorAll(".highlight").forEach(e=>createCopyButton(e,script)),document.body.appendChild(script)}</script></body></html>